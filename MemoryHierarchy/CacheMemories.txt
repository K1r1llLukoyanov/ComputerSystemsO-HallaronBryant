        Cache Memories

    The memory hierarchies of early computer systems consisted of only three levels: CPU registers, main memory, and
    disk storages. However, bacause of the increasing gap between CPU and main memory, system designers were compelled
    to insert a small SRAM cache memory, called an L1 cache between CPU register file and main memory. L1 cache can be
    accessed nearly as fast as the registers, typically in about 4 block cycles. As performance gap between the CPU and
    main memory continued to increase, system designers responded by inserting and additional larger cache, called L2
    cache, between L1 cache and main memory, that can be accessed in about 10 clock cycles. Many modern systems include
    an even larger cache, called an L3 cache, which sits between L2 cache and main memory in memory hierarchy and can be
    accessed in about 50 clock cycles.

    
    Generic Cache Memory Organization

    Consider a computer system where each memory address has m bits that form M=2^m unique addresses. A cache for such a
machine is organized as an arrays of S=2^s cache sets. Each set consists of E cache lines. Each lines consists of a data
block of B=2^b bytes, a valid bit that indicates whether or not the line consists meaningful information, and t = m - (b + s)
tag bits (a subset of the bits from the current block's memory address) that uniquely identify the block stored in the cache.
    In general, a cache's organization can be characterized by the tuple (S, E, B, m). The size (or capacity) of a cache, C, 
is stated in terms of the aggregate size of all the blocks. The tag bits and valid bit are not included. Thus, C = S*B*E.
    When the CPU is instructed by a load instruction to read a word from address A of main memory, it sends the address A
to the cache. If the cache is holding a copy of the word at address A, it sends the word immediately back to the CPU. The
cache is organized so that it can find the requested word by simply inspecting the bits of the address, similar to a hash
table with an extemely simple hash function. Here how it works:
    The parameters S and B include a partitioning of the m address bits into the three fields. The s set index bits in A
form an index into the array of S sets. The first set is set 0, the second is set 1, and so on. When interpreted as an
unsigned integer, the set index bits tell us which set the word must be stored in. Once we know which set the word must be
contained in, the t tag bits in A tell us which line (if any) in set contains the word. A line in the set contains the word
if and only if the valid bit is set and the tag bits in the line match the tag bits in the address A. Once we have located
the line identified by the tag in the set identified by the set index, then the b block offset bits give us the offset of
the word in the B-byte data block.

Practice Problem 6.9 (solution page 699)

The following table gives the parameters for a number of different caches. For
each cache, determine the number of cache sets (S), tag bits (t), set index bits (s),
and block offset bits (b).

Cache       m           C           B           E           S           t           s           b
1           32          1024        4           1           256        22           8           2
2           32          1024        8           4           32         24           5           3
3           32          1024        32          32          1          27           0           5




Summary of cache parameters:
---------------------------------------------------------------------------------------------------------
Parameters                  Description
---------------------------------------------------------------------------------------------------------
Fundamental parameters
S = 2^s                     Number of sets
E                           Number of lines per set
B = 2^b                     Block size(bytes)
m = log(M)                  Number of physical (main memory) address bits

Derived quantities
M = 2^m                     Maximum number of unique memory addresses
s = log(S)                  Number of set index bits
b = log(B)                  Number of block offset bits
t = m - (s + b)             Number of tag bits
C = B*S*E                   Cache size(byts), not including overhead such as the valid and tag bits
---------------------------------------------------------------------------------------------------------



    Direct-Mapped Caches

    Cache are grouped into different classes based on E, the number of cache lines per set. A cache with exactly one line
per set is known as direct-mapped cache.
    Suppose we have a system with a CPU, a register file, an L1 cache, and a main memory. When the CPU executes as instruction
that reads a memory word w, it requests the word from L1 cache. If the cache has a copy of w, then we have cache hit, and L1
cache quickly extracts w and return it back to the CPU. Otherwise, we have cache miss, and the CPU must wait while the L1 requests
a copy of block containing w from the main memory. When the requested block finally arrives from memory, the L1 cache stores the
block in one of its cache lines, extract word w from the stored block and returns it to the CPU. The process that a cache goes
through of determining whether a request is a hit or miss and then extracting the requested word consists of three steps: (1) set
selection, (2) line matching and (3) word extraction.

    Set selection in direct-mapped caches

    In this step, the cache extracts the s set index bits from the middle of the address for w. These bits are interpreted as an
unsigned integer that correspond to a set number. In other words, if we think of the cache as a one-dimensional array of sets, then
set index bits form as index into this array.

    Line matching in Direct-Mapped Caches

    The next step is to determine if a copy of word w is stored in one of the cache lines contained in set i. In direct-mapped cache this
is easy and fast because there is exactly one line per set. A copy of w is contained in the line if and only if the valid bit is set and
the tag in the cache line matches the tag in the address of w.

    Word Selection in Direct-Mapped Caches

    Once we have a hit, we know that w is somewhere in block. This is last step determines where the desired word starts in the block. The
block offset bits provide us with the offset of the first byte in the desired word. Similar to our view of a cache as an array of lines, we
can think of block as an array of bytes, and the byte offset as an index into that array. If block offset bits of 100 indicates that the copy
of w starts at byte 4 in the block.

    Line Replacement on Misses in Direct-Mapped Cache

    If the cache misses, then it needs to retrieve the requested block from the next level in the memory hierarchy and store the new block
in one of the cache lines of the set indicated by the set index bits. In general, if the set is full of valid cache lines, then one of the
existing lines must be evicted. For a direct-mapped cache, where each set contains exactly one line, the replacement policy is trivial, the
current line is relaced by the newly fetched line.

    Putting It Together: A Direct-Mapped Cache in Action    

    The mechanisms that a cache uses to select sets and identify lines are extemely simple. They have to be, because the hardware must
perform them in few nanoseconds. However, manipulating bits in this way can be confusing to us humans. Suppose we have a direct-mapped 
cache described by:
                (S, E, B, m) = (4, 1, 2, 4)
In other words, the cache has four sets, one line per set, 2 bytes per block, and 4-bit addresses.

  * The concatenation of the tag and index bits uniquely identifies each block in memory. For example, block 0 consists of addresses
    0 and 1, block 1 consists of addresses 2 and 3, and so on.
  * Since there are eight memory blocks but only four cache sets, multiple blocks map to the same cache set ( they have the same cache index)
    For example, blocks 0 and 4 both mapped to set 0, blocks 1 and 5 both mapped to set 1, and so on.
  * Blocks that map to the same cache set are uniquely identified by the tag. For example, block 0 has a tag bit of 0 while block 4 has
    a tag bit of 1, and so on.



    4-bits address space for example direct-mapped cache.

    --------------------------------------------------------------------------------------------------------
                                                Address bits
                            ----------------------------------------------------
    Address                 Tag                 Index bits              Offset bits             Block number
    (decimal)               bits                (s = 2)                 (b = 1)                   (decimal)
    --------------------------------------------------------------------------------------------------------
        0                   0                       00                      0                           0
        1                   0                       00                      1                           0
        2                   0                       01                      0                           1
        3                   0                       01                      1                           1
        4                   0                       10                      0                           2
        5                   0                       10                      1                           2
        6                   0                       11                      0                           3
        7                   0                       11                      1                           3
        8                   1                       00                      0                           4
        9                   1                       00                      1                           4
        10                  1                       01                      0                           5
        11                  1                       01                      1                           5
        12                  1                       10                      0                           6
        13                  1                       10                      1                           6
        14                  1                       11                      0                           7
        15                  1                       11                      1                           7
    
    
    Let's simulate the cache in action as the CPU performs a sequence of reads.
    
    Initially, the cache is empty (i.e., each valid bit is 0)
    
    Set         Valid           Tag         block[0]            block[1]         
    0           0
    1           0
    2           0
    3           0
    
    Each row in the table represents a cache line. The first column indicates the set that the line belongs to.
    The next found columns represents the actual bits in each cache line.

    1.  Read word at address 0. Since the valid bit for set 0 is 0, this is a cache miss. The cache fetches block 0
        from memory (or lower-level cache) and stores the block in set 0. Then the cache returns m[0] (the content of
        memory location 0) from block[0] of the newly fetched cache line.

    Set         Valid           Tag         block[0]            block[1]         
    0           1               0           m[0]                    m[1]
    1           0
    2           0
    3           0  
    
    2.  Read word at address 1. This is a cache hit. The cache immediately returns m[1] from block[1] of the cache
        line. The state of the cache does not change.
    
    3.  Read word at address 13. Since the cache line in set 2 is not valid, this is a cache miss. The cache loads
        the block 6 into set 2 and returs m[13] from block[1] of the new fetched cache line.

    Set         Valid           Tag         block[0]            block[1]         
    0           1               0               m[0]            m[1]
    1           0
    2           1               1               m[12]           m[13]            
    3           0
    
    4.  Raad word at address 8. This is miss. The cache line in set 0 is indeed valid, but the tags do not match. The
        cache loads block 4 into set 0 (replacing the line that was there from the read of address 0) and returns m[8]
        from block[0] of the new cache line.

    Set         Valid           Tag         block[0]            block[1]         
    0           1               1           m[8]                m[9]
    1           0
    2           1               1           m[12]               m[13]
    3           0
    
    5.  Read word at address 0. This is another miss, due to the unfornutable fact that we just replaced block 0 during
        the previous reference to address 8. This kind of miss, where we have plenty of room in the cache but keep alternating
        reference to block that map to the same set, in an example of a conflict miss.

    Set         Valid           Tag         block[0]            block[1]         
    0           1               0           m[0]                m[1]
    1           0
    2           1               1           m[12]               m[13]
    3           0

    
    Conflict Misses in Direct-Mapped Caches
    
    Conflict misses are common in real programs and can issue baffling performance problems. Conflict misses in direct-mapped
    caches typically occurs when program access arrays whose sizes are power of 2. Consider a function:
    
    1   float dotprod(float x[8], float y[8])
    2   {
    3       float sum = 0.0;
    4       int i;
    5
    6       for (i = 0; i < 8; i++)
    7           sum += x[i] * y[i];
    8       return sum;
    9   }

    This function has good spacial locality with respect to x and y, and so we might expect it to enjoy a good number of cache hits.
    Unfortunately it is not always true.
    
    Suppose that floats are 4 bytes, that x is loaded into the 32 bytes of contiguous memory starting at address 0, and that y starts
immediately after x at address 32. For simplicity, suppose a block is 16 bytes (big enough to hold four floats). And that the cache
consists of two sets, for total cache size is 32 bytes. We assume that variable sum is stored in a CPU register. 

    Element         Address         Set index                       Element         Address         Set index
    x[0]            0               0                               y[0]            32              0
    x[1]            4               0                               y[1]            36              0
    x[2]            8               0                               y[2]            40              0
    x[3]            12              0                               y[3]            44              0
    x[4]            16              1                               y[4]            48              1
    x[5]            20              1                               y[5]            52              1
    x[6]            24              1                               y[6]            56              1
    x[7]            28              1                               y[7]            60              1

    At run time, the first iteration of the loop references x[0], a miss that causes the block containing x[0]-x[3] to be loaded
into set 0. The next reference is to y[0], another miss that causes the block containing y[0]-y[3] to be loaded into set 0, overwriting
the values of x what were copied in by the previous reference. So now we have conflict miss, and in fact each subsequent reference to x
and y will result in a conflict miss as we thrash back and forth between blocks of x and y. The term thrashing describes any situation
where a cache is repeatedly loading and evicting the same sets of cache blocks.
    Luckily, thrashing is easy for programmers to fix once they recognized what is going on. One easy solution is to put B bytes at the
end of each array.
    For example, instead of defining x to be float x[8], we define it to be float x[12]. Assuming y starts immediately after x in memory,
we have the following mapping of array elements to sets:

    Element         Address         Set index                       Element         Address         Set index
    x[0]            0               0                               y[0]            48              1
    x[1]            4               0                               y[1]            52              1
    x[2]            8               0                               y[2]            56              1
    x[3]            12              0                               y[3]            60              1
    x[4]            16              1                               y[4]            64              0
    x[5]            20              1                               y[5]            68              0
    x[6]            24              1                               y[6]            72              0
    x[7]            28              1                               y[7]            76              0

    With the paddling at the end of x, x[i] and y[i] now map to different sets, which eliminates the thrashing conflict misses.



    Practice Problem 6.10 (solution page 699)

    In the previous dotprod example, what fraction of the total references to x and y
    will be hits once we have padded array x? 

    Answer: 12/16 = 6/8 = 0.75 (75%)
    

    Practice Problem 6.11 (solution page 699)

    Imagine a hypothetical cache that uses the high-order s bits of an address as the
    set index. For such a cache, contiguous chunks of memory blocks are mapped to
    the same cache set.

    A.  How many blocks are in each of these contiguous array chunks? : 2^s
    B.  (S, E, B, m) = (512, 1, 32, 32)
        
        int array[4096];

        for(i = 0; i < 4096; i++)
            sum += array[i]
        
        What is the maximum number of array blocks that are stored in the cache
        at any point in time?
        
        The cache capacity is 512 32-byte blocks with t = 32 - (9 + 5) = 18 bits in each cache line.
        Thus, first 2^18 blocks in the array will be mapped to set 0, the next 2^18 blocks to set 1.
        Our array consists of only (4096*4)/32 = 512 blocks, all of the blocks in the array map to
        set 0. Thus, the cache will hold at most 1 array block at any point in time.

    
    Set Associative Caches

    The problem with conflict misses in direct-mapped caches stems from the constraint that each set has exactly one line (E=1).
A set associative caches relaxes this constraint so that each set hold more than one cache line. A cache with 1 < E < C/B is often
called E-way set associative cache.

    Set Selection in Set Associative Caches
    
    Set selection is identical to a direct-mapped cache, with the set index bits identifying the set.

    Line Matching and Word Selection in Set Associative Cache

    Line matching is more involved in a set associative cache than in a direct-mapped cache because it must check the tags and valid
bits of multiple lines in order to determine if the requested word is in the set. A convintional memory is an array of values that
takes an address and return a value stored at this address. An associative memory, on the other hand, is an array of (key, value) pairs
that takes as input the key and returns a value from one of the (key, value) pairs that matches the input key. Thus, we can think of
each set associative cache as a small associative memory where the keys are the concatenation of the tag and valid bits, and the
values are contents of a block. An important idea here is that any line in the set can contain any of the memory blocks that map to
that set. So the cache must search each line in the set for a valid line whose tag matches the tag in the address. If cache finds
such a line, then we have a hit and block offset selects a word from the block.

    Line Replacing on Misses in Set Associative Caches

    If the word requested by the CPU is not stored in any block of the lines in the set, then we have a cache miss, and the cache
must fetch the block that contains the word from memory. If there is an empty line, then it would be a good condidate. But if there
are no empty lines in the set, then we must choose of the nonempty lines and hope that the CPU does not reference the replaced line
anytime soon.
    The simplest replacement pocily is to choose the line to replace at random. Other more sophisticated policies draw on priciple
of locality to try to minimize the probability that the replaced line will be referenced in the near feature. For example, least
frequently used (LFU) policy will replace the line that has been referenced the fewest time over some past time window. A least
recently used (LRU) policy will replace the line that was last accesed the furthest in the past. All of these policies require 
additional time and hardware. But as we move further down the memory hierarchy, away from the CPU, the cost of a miss becomes more
expensive and it becomes more worthwhile to minimize misses with good replacement policies.

    
    Fully Associative Caches
    
    A fully associative cache consists of a single set (E = C/B) that contains all of the cache lines. 

    Set Selection in Fully Associative Caches
    
    Set selection in fully associative cache is trivial because here is only one set. We don't need set index bits in the address.

    Line Matching and Word Selection in Fully Associative Cache

    Line matching and word selection in fully associative cache works the same as with a set associative cache. The difference is
mainly a question of slace.
    Because the cache circuitry must search for many matching tags in parallel, it is difficult and expensive to build an associative
cache that is both large and fast. As a result, fully associated caches are only appropriate for small caches, such as the translation
lookaside buffers (TLBs) in virtual memory systems that cache page table entries. 

    Practice Problem 6.12 (solution page 699)
    
    The problems that follow will help reinforce your understanding of how caches
    work. Assume the following:

    *   The memory is byte addresable.
    *   The memory accesses are to 1-byte words (not 4-bytes words).
    *   Addresses are 13 bits wide.
    *   The cache is two-way associative (E=2), with 4-byte block size (B = 4) and eight sets (S = 8)
    
    The contents of the cache are as follows, with all numbers given in hexadecimal
    notation.

                                                2-way set associative cache
    --------------------------------------------------------------------------------------------------------------
                                    Line 0                                              Line 1
                ----------------------------------------------      ----------------------------------------------
    Set index   Tag     Valid   Byte 0  Byte 1  Byte 2  Byte 3      Tag     Valid   Byte 0  Byte 1  Byte 2  Byte 3
    0           09      1       86      30      3F      10          00      0       —       —       —       —
    1           45      1       60      4F      E0      23          38      1       00      BC      0B      37
    2           EB      0       —       —       —       —           0B      0       —       —       —       —
    3           06      0       —       —       —       —           32      1       12      08      7B      AD
    4           C7      1       06      78      07      C5          05      1       40      67      C2      3B
    5           71      1       0B      DE      18      4B          6E      0       —       —       —       —
    6           91      1       A0      B7      26      2D          F0      0       —       —       —       —
    7           46      0       —       —       —       —           DE      1       12      C0      88      37
    --------------------------------------------------------------------------------------------------------------

    The following figure shows the format of an address (1 bit per box). Indicate
    (by labeling the diagram) the fields that would be used to determine the following:

    CO. The cache block offset
    CI. The cache set index
    CT. The cache tag

    CT  CT  CT  CT  CT  CT  CT  CT  CI  CI  CI  CO  CO
    12  11  10  9   8   7   6   5   4   3   2   1   0 

    
    Practice Problem 6.13 (solution page 700)
    
    Suppose a program running on the machine in Problem 6.12 references the 1-byte
    word at address 0x0D53. Indicate the cache entry accessed and the cache byte
    value returned in hexadecimal notation. Indicate whether a cache miss occurs. If
    there is a cache miss, enter “—” for “Cache byte returned.”
    
    0x0D53 -> 0000 1101 0101 0011

    A. Address format (1 bit per box):
    
    0   1   1   0   1   0   1   0   1   0   0   1   1
    12  11  10  9   8   7   6   5   4   3   2   1   0
    
    B. Memory reference:
    
    Parameter                   Value
    Cache block offset (CO)     0x3
    Cache set index (CI)        0x4
    Cache Tag (CT)              0x6A
    Cache hit? (Y/N)            N
    Cache byte returned         --
    
    
    Practice Problem 6.14 (solution page 700)
    
    Repeat Problem 6.13 for memory address 0x0CB4:
    
    A. Address format (1 bit per box):
    
    0   1   1   0   0   1   0   1   1   0   1   0   0
    12  11  10  9   8   7   6   5   4   3   2   1   0   
    
    B. Memory reference:
    
    Parameter                   Value
    Cache block offset (CO)     0x0
    Cache set index (CI)        0x5
    Cache Tag (CT)              0x65
    Cache hit? (Y/N)            N
    Cache byte returned         --

    
    Practice Problem 6.15 (solution page 700)
    Repeat Problem 6.13 for memory address 0x0A31
    
    A. Address format (1 bit per box):
    
    0   1   0   1   0   0   0   1   1   0   0   0   1
    12  11  10  9   8   7   6   5   4   3   2   1   0

    B. Memory reference:

    Parameter                   Value
    Cache block offset (CO)     0x1
    Cache set index (CI)        0x4
    Cache Tag (CT)              0x51
    Cache hit? (Y/N)            N
    Cache byte returned         --

    

    Practice Problem 6.16 (solution page 701)
    32
    0   0   1   1   0   0   1   0   0   1   1   0   0   ->  0x064C
    0   0   1   1   0   0   1   0   0   1   1   0   1   ->  0x064D
    0   0   1   1   0   0   1   0   0   1   1   1   0   ->  0x064E
    0   0   1   1   0   0   1   0   0   1   1   1   1   ->  0x064F
    
    
    
    Issues with Writes

    The situations for writes is a little more complicated than for reads. Suppose we write a word w that is already cached
(a write bit). After the cache updates its copy of w, what does it do about updating the copy of w in the next lower level of
the hierarchy? The simplest approach, known as write-through, is to immediately write w's cache block to the next lower level memory.
While simple, write-throught has the disadvantage of causing bus traffic with every write. Another approach, known as write-back,
defers the update as long as possible by writing the updated block to the next lower level only when it evicted from the cache by
the replacement algorithm. Because of locality, write-back can significantly reduce the amount of bus traffic, but is has the
disadvantage of additional complexity. The cache must maintain an additional dirty bit for each cache line, that indicates whether
or nor the cache block has been modified.
    Another issue is how to deal with write misses. One approach, known as write-allocate, loads the corresponding block from the next
lower level into the cache and then updates the cache block. Write-allocate tries to exploit spacial locality of writes, but it has
the disadvantage that every miss result in a block transfer from the next lower level to the cache. The alternative, known as no-write-
allocate, bypasses the cache and writes the word directly to the next lower level. Write-through caches are typically no-write-allocate.
Write-back are typically write-allocate.

    
    Anatomy of a Real Cache Hierarchy
    
    Caches can hold both program data and program instructions. A cache that holds instructions only is called i-cache. A cache
that holds program data only is called d-cache. A cache that holds both instructions and data is known as unified cache. Modern
processors include two separate i-caches and d-caches. There are a number of reasons for this. With two separate caches, the
processor can read an instruction word and data word at the same time. I-caches are typically read-only, and thus simpler. The
two caches are often optimized to different access patterns and can have different block sizes, associativities, and capacities.
Also, having separate caches ensures that data accesses do not create conflict misses with instruction accesses, and vice verse,
at the cost of a potential increase in capacities misses.


    Characteristics of the Intel Core i7 cache hierarchy.
    
    ------------------------------------------------------------------------------------------------------------
    Cache type      Access time (cycles)        Cache size (C)      Assoc. (E)      Block size (B)      Sets (S)
    ------------------------------------------------------------------------------------------------------------
    L1 i-cache                  4                   32KB                8               64B                 64
    L1 d-cache                  4                   32KB                8               64B                 64
    L2 unified cache           10                  256KB                8               64B                512
    L3 unified cache        40-75                    8MB               16               64B               8192
    ------------------------------------------------------------------------------------------------------------

    

    Performance Impact of Cache Parameters

    Cache performance is evaluated with a number of metrics:
    
      * Miss rate:  The fraction of memory references during the execution of a program, or a part of a program, that miss.
                    It is computed as #misses/#references.
      * Hit rate:   The fraction of memory references that hits. It computed as 1 - miss rate.
      * Hit time:   The time to deliver a word in the cache to the CPU, including the time for set selection, line identification,
                    and word selection. Hit time is on the order of several clock cycles for L1 caches.
      * Miss penalty:   Any additional time required because of miss. The penalty for L1 misses served from L2 is on the order of
                        10 cycles; from L3, 50 cycles; and from main memory, 200 cycles.

    
    Impact of Cache Size

    On one hand, a larger cache will tend to increase the hit rate. On the other hand, it is always harder to make large memories
run faster. As a result, larger caches tend to increase the hit time. This explains why an L1 cache is smaller than L2, and L2
cache is smaller than L3 cache.

    Impact of Block Size
    
    Large blocks are mixed blessing. One the one hand, larger blocks can help increase the hit rate by exploiting any spacial
locality that might exist in a program. However, for a given cache size, larger blocks imply a smaller number of cache lines,
which can hurt the hit rate in programs with more temporal locality than spacial locality. Larger blocks also have a negative
impact on the miss penalty, since largers blocks cause larger transfer times. Modern systems such as the Core i7 compromise
with cache blocks that contains 64 bytes.

    Impact of Associativity

    The issue here is the impact of the choice of the parameter E, the number of cache lines per set. The advantage of higher
associativity (larger values of E) is that it decreases the vulnerability of the cache to thrashing due to conflict misses.
However, higher associativity comes at a significant cost. Higher associativity is expensive to implement and hard to make
fast. It requires more tag bits per line, additional LRU state bits per line, and additional contol logic. Higher associativity
can increase hit time, because of the increased complexity, and it can also increase the miss penalty because of the increased
complexity of choosing a victim line.
    The choice of assiciativity ultimately boils down to trade-off between the hit time and miss penalty. Traditionally, high-
perforfance systems that pushed the clock rates would opt for smaller associativity for L1 caches (where the miss penalty is
only a few cycles) and higher degree of assiciativity for the lower levels, where the miss penalty is higher. For example, an
Intel Core i7 systems, the L1 and L2 caches are 8-way associative, and the L3 cache is 16-way.

    Impact of Write Strategy

    Write-through caches are simpler to implement and can use a write buffer that works independently of the cache to
update memory. Furthermore, read misses are less expensive bacause they do not trigger a memory write. On the other
hand, write-back caches result in fewer transfer, which allows more bandwidth to memory for I/O devices that perform DMA.
Further, reducing the number of transfer times increasingly important as we move down the hierarchy and the transfer times
increase. In general, caches further down the heirarchy are more likely to use write-back than write-through.

