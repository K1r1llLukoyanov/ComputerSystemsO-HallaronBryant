        Cache Memories

    The memory hierarchies of early computer systems consisted of only three levels: CPU registers, main memory, and
    disk storages. However, bacause of the increasing gap between CPU and main memory, system designers were compelled
    to insert a small SRAM cache memory, called an L1 cache between CPU register file and main memory. L1 cache can be
    accessed nearly as fast as the registers, typically in about 4 block cycles. As performance gap between the CPU and
    main memory continued to increase, system designers responded by inserting and additional larger cache, called L2
    cache, between L1 cache and main memory, that can be accessed in about 10 clock cycles. Many modern systems include
    an even larger cache, called an L3 cache, which sits between L2 cache and main memory in memory hierarchy and can be
    accessed in about 50 clock cycles.

    
    Generic Cache Memory Organization

    Consider a computer system where each memory address has m bits that form M=2^m unique addresses. A cache for such a
machine is organized as an arrays of S=2^s cache sets. Each set consists of E cache lines. Each lines consists of a data
block of B=2^b bytes, a valid bit that indicates whether or not the line consists meaningful information, and t = m - (b + s)
tag bits (a subset of the bits from the current block's memory address) that uniquely identify the block stored in the cache.
    In general, a cache's organization can be characterized by the tuple (S, E, B, m). The size (or capacity) of a cache, C, 
is stated in terms of the aggregate size of all the blocks. The tag bits and valid bit are not included. Thus, C = S*B*E.
    When the CPU is instructed by a load instruction to read a word from address A of main memory, it sends the address A
to the cache. If the cache is holding a copy of the word at address A, it sends the word immediately back to the CPU. The
cache is organized so that it can find the requested word by simply inspecting the bits of the address, similar to a hash
table with an extemely simple hash function. Here how it works:
    The parameters S and B include a partitioning of the m address bits into the three fields. The s set index bits in A
form an index into the array of S sets. The first set is set 0, the second is set 1, and so on. When interpreted as an
unsigned integer, the set index bits tell us which set the word must be stored in. Once we know which set the word must be
contained in, the t tag bits in A tell us which line (if any) in set contains the word. A line in the set contains the word
if and only if the valid bit is set and the tag bits in the line match the tag bits in the address A. Once we have located
the line identified by the tag in the set identified by the set index, then the b block offset bits give us the offset of
the word in the B-byte data block.

Practice Problem 6.9 (solution page 699)

The following table gives the parameters for a number of different caches. For
each cache, determine the number of cache sets (S), tag bits (t), set index bits (s),
and block offset bits (b).

Cache       m           C           B           E           S           t           s           b
1           32          1024        4           1           256        22           8           2
2           32          1024        8           4           32         24           5           3
3           32          1024        32          32          1          27           0           5




Summary of cache parameters:
---------------------------------------------------------------------------------------------------------
Parameters                  Description
---------------------------------------------------------------------------------------------------------
Fundamental parameters
S = 2^s                     Number of sets
E                           Number of lines per set
B = 2^b                     Block size(bytes)
m = log(M)                  Number of physical (main memory) address bits

Derived quantities
M = 2^m                     Maximum number of unique memory addresses
s = log(S)                  Number of set index bits
b = log(B)                  Number of block offset bits
t = m - (s + b)             Number of tag bits
C = B*S*E                   Cache size(byts), not including overhead such as the valid and tag bits
---------------------------------------------------------------------------------------------------------



    Direct-Mapped Caches

    Cache are grouped into different classes based on E, the number of cache lines per set. A cache with exactly one line
per set is known as direct-mapped cache.
    Suppose we have a system with a CPU, a register file, an L1 cache, and a main memory. When the CPU executes as instruction
that reads a memory word w, it requests the word from L1 cache. If the cache has a copy of w, then we have cache hit, and L1
cache quickly extracts w and return it back to the CPU. Otherwise, we have cache miss, and the CPU must wait while the L1 requests
a copy of block containing w from the main memory. When the requested block finally arrives from memory, the L1 cache stores the
block in one of its cache lines, extract word w from the stored block and returns it to the CPU. The process that a cache goes
through of determining whether a request is a hit or miss and then extracting the requested word consists of three steps: (1) set
selection, (2) line matching and (3) word extraction.

    Set selection in direct-mapped caches

    In this step, the cache extracts the s set index bits from the middle of the address for w. These bits are interpreted as an
unsigned integer that correspond to a set number. In other words, if we think of the cache as a one-dimensional array of sets, then
set index bits form as index into this array.

    Line matching in Direct-Mapped Caches

    The next step is to determine if a copy of word w is stored in one of the cache lines contained in set i. In direct-mapped cache this
is easy and fast because there is exactly one line per set. A copy of w is contained in the line if and only if the valid bit is set and
the tag in the cache line matches the tag in the address of w.

    Word Selection in Direct-Mapped Caches

    Once we have a hit, we know that w is somewhere in block. This is last step determines where the desired word starts in the block. The
block offset bits provide us with the offset of the first byte in the desired word. Similar to our view of a cache as an array of lines, we
can think of block as an array of bytes, and the byte offset as an index into that array. If block offset bits of 100 indicates that the copy
of w starts at byte 4 in the block.

    Line Replacement on Misses in Direct-Mapped Cache

    If the cache misses, then it needs to retrieve the requested block from the next level in the memory hierarchy and store the new block
in one of the cache lines of the set indicated by the set index bits. In general, if the set is full of valid cache lines, then one of the
existing lines must be evicted. For a direct-mapped cache, where each set contains exactly one line, the replacement policy is trivial, the
current line is relaced by the newly fetched line.

    Putting It Together: A Direct-Mapped Cache in Action    

    The mechanisms that a cache uses to select sets and identify lines are extemely simple. They have to be, because the hardware must
perform them in few nanoseconds. However, manipulating bits in this way can be confusing to us humans. Suppose we have a direct-mapped 
cache described by:
                (S, E, B, m) = (4, 1, 2, 4)
In other words, the cache has four sets, one line per set, 2 bytes per block, and 4-bit addresses.

  * The concatenation of the tag and index bits uniquely identifies each block in memory. For example, block 0 consists of addresses
    0 and 1, block 1 consists of addresses 2 and 3, and so on.
  * Since there are eight memory blocks but only four cache sets, multiple blocks map to the same cache set ( they have the same cache index)
    For example, blocks 0 and 4 both mapped to set 0, blocks 1 and 5 both mapped to set 1, and so on.
  * Blocks that map to the same cache set are uniquely identified by the tag. For example, block 0 has a tag bit of 0 while block 4 has
    a tag bit of 1, and so on.



    4-bits address space for example direct-mapped cache.

    --------------------------------------------------------------------------------------------------------
                                                Address bits
                            ----------------------------------------------------
    Address                 Tag                 Index bits              Offset bits             Block number
    (decimal)               bits                (s = 2)                 (b = 1)                   (decimal)
    --------------------------------------------------------------------------------------------------------
        0                   0                       00                      0                           0
        1                   0                       00                      1                           0
        2                   0                       01                      0                           1
        3                   0                       01                      1                           1
        4                   0                       10                      0                           2
        5                   0                       10                      1                           2
        6                   0                       11                      0                           3
        7                   0                       11                      1                           3
        8                   1                       00                      0                           4
        9                   1                       00                      1                           4
        10                  1                       01                      0                           5
        11                  1                       01                      1                           5
        12                  1                       10                      0                           6
        13                  1                       10                      1                           6
        14                  1                       11                      0                           7
        15                  1                       11                      1                           7
    
    
    Let's simulate the cache in action as the CPU performs a sequence of reads.
    
    Initially, the cache is empty (i.e., each valid bit is 0)
    
    Set         Valid           Tag         block[0]            block[1]         
    0           0
    1           0
    2           0
    3           0
    
    Each row in the table represents a cache line. The first column indicates the set that the line belongs to.
    The next found columns represents the actual bits in each cache line.

    1.  Read word at address 0. Since the valid bit for set 0 is 0, this is a cache miss. The cache fetches block 0
        from memory (or lower-level cache) and stores the block in set 0. Then the cache returns m[0] (the content of
        memory location 0) from block[0] of the newly fetched cache line.

    Set         Valid           Tag         block[0]            block[1]         
    0           1               0           m[0]                    m[1]
    1           0
    2           0
    3           0  
    
    2.  Read word at address 1. This is a cache hit. The cache immediately returns m[1] from block[1] of the cache
        line. The state of the cache does not change.
    
    3.  Read word at address 13. Since the cache line in set 2 is not valid, this is a cache miss. The cache loads
        the block 6 into set 2 and returs m[13] from block[1] of the new fetched cache line.

    Set         Valid           Tag         block[0]            block[1]         
    0           1               0               m[0]            m[1]
    1           0
    2           1               1               m[12]           m[13]            
    3           0
    
    4.  Raad word at address 8. This is miss. The cache line in set 0 is indeed valid, but the tags do not match. The
        cache loads block 4 into set 0 (replacing the line that was there from the read of address 0) and returns m[8]
        from block[0] of the new cache line.

    Set         Valid           Tag         block[0]            block[1]         
    0           1               1           m[8]                m[9]
    1           0
    2           1               1           m[12]               m[13]
    3           0
    
    5.  Read word at address 0. This is another miss, due to the unfornutable fact that we just replaced block 0 during
        the previous reference to address 8. This kind of miss, where we have plenty of room in the cache but keep alternating
        reference to block that map to the same set, in an example of a conflict miss.

    Set         Valid           Tag         block[0]            block[1]         
    0           1               0           m[0]                m[1]
    1           0
    2           1               1           m[12]               m[13]
    3           0

    
    Conflict Misses in Direct-Mapped Caches
    
    Conflict misses are common in real programs and can issue baffling performance problems. Conflict misses in direct-mapped
    caches typically occurs when program access arrays whose sizes are power of 2. Consider a function:
    
    1   float dotprod(float x[8], float y[8])
    2   {
    3       float sum = 0.0;
    4       int i;
    5
    6       for (i = 0; i < 8; i++)
    7           sum += x[i] * y[i];
    8       return sum;
    9   }

    This function has good spacial locality with respect to x and y, and so we might expect it to enjoy a good number of cache hits.
    Unfortunately it is not always true.
    
    Suppose that floats are 4 bytes, that x is loaded into the 32 bytes of contiguous memory starting at address 0, and that y starts
immediately after x at address 32. For simplicity, suppose a block is 16 bytes (big enough to hold four floats). And that the cache
consists of two sets, for total cache size is 32 bytes. We assume that variable sum is stored in a CPU register. 

    Element         Address         Set index                       Element         Address         Set index
    x[0]            0               0                               y[0]            32              0
    x[1]            4               0                               y[1]            36              0
    x[2]            8               0                               y[2]            40              0
    x[3]            12              0                               y[3]            44              0
    x[4]            16              1                               y[4]            48              1
    x[5]            20              1                               y[5]            52              1
    x[6]            24              1                               y[6]            56              1
    x[7]            28              1                               y[7]            60              1

    At run time, the first iteration of the loop references x[0], a miss that causes the block containing x[0]-x[3] to be loaded
into set 0. The next reference is to y[0], another miss that causes the block containing y[0]-y[3] to be loaded into set 0, overwriting
the values of x what were copied in by the previous reference. So now we have conflict miss, and in fact each subsequent reference to x
and y will result in a conflict miss as we thrash back and forth between blocks of x and y. The term thrashing describes any situation
where a cache is repeatedly loading and evicting the same sets of cache blocks.
    Luckily, thrashing is easy for programmers to fix once they recognized what is going on. One easy solution is to put B bytes at the
end of each array.
    For example, instead of defining x to be float x[8], we define it to be float x[12]. Assuming y starts immediately after x in memory,
we have the following mapping of array elements to sets:

    Element         Address         Set index                       Element         Address         Set index
    x[0]            0               0                               y[0]            48              1
    x[1]            4               0                               y[1]            52              1
    x[2]            8               0                               y[2]            56              1
    x[3]            12              0                               y[3]            60              1
    x[4]            16              1                               y[4]            64              0
    x[5]            20              1                               y[5]            68              0
    x[6]            24              1                               y[6]            72              0
    x[7]            28              1                               y[7]            76              0

    With the paddling at the end of x, x[i] and y[i] now map to different sets, which eliminates the thrashing conflict misses.



    Practice Problem 6.10 (solution page 699)

    In the previous dotprod example, what fraction of the total references to x and y
    will be hits once we have padded array x? 

    Answer: 12/16 = 6/8 = 0.75 (75%)
    

    Practice Problem 6.11 (solution page 699)

    Imagine a hypothetical cache that uses the high-order s bits of an address as the
    set index. For such a cache, contiguous chunks of memory blocks are mapped to
    the same cache set.

    A.  How many blocks are in each of these contiguous array chunks? : 2^s
    B.  (S, E, B, m) = (512, 1, 32, 32)
        
        int array[4096];

        for(i = 0; i < 4096; i++)
            sum += array[i]
        
        What is the maximum number of array blocks that are stored in the cache
        at any point in time?
        
        The cache capacity is 512 32-byte blocks with t = 32 - (9 + 5) = 18 bits in each cache line.
        Thus, first 2^18 blocks in the array will be mapped to set 0, the next 2^18 blocks to set 1.
        Our array consists of only (4096*4)/32 = 512 blocks, all of the blocks in the array map to
        set 0. Thus, the cache will hold at most 1 array block at any point in time.