        The memory hierarchy

    Chaching in the Memory Hierarchy

    In general, a cache is small, fast storage device that acts as a staging area for the data object stored in a larger, slower
device. The process of using a cache is known as chaching.
    The central idea of a memory hierarchy is that for each k, the faster and smaller storage device at level k serves as a cache
for a larger and slower storage devices at level k+1.
    Data are always copied back and forth between level k and k+1 in block-size transfer unit.

    Cache Hits.

    When a program need a particular data object d from level k+1, it first looks for d in one of the blocks currently
stored at level k. If d happens to be cached at level k, then we have what is called cache hit. The program reads d
directly from level k, which by the nature of the memory hierarchy is faster than reading d from level k+1.

    Cache Misses

    If, one other hand, the data object d is not cached at level k, then we have what is called cache miss. When there is a miss,
the cache at level k fetches the block containing d from the cache level k+1, possibly overwritting an existing block if the level
k cache is already full. 
    The process of overwritting an existing block is known as replacing or evicting the block. The block that is evicted is sometimes
referred as a victim block. The decision about which block to replace is governed by the cache's replacement policy. For example, a
cache with a random replacement policy would choose a random random victim block. A cache with least recently used (LRU) replacement
policy would choose the block that was last accessed the furthest in the past.
    After the cache at level k has fetched the block from level k+1, the program can read d from level k as before.

    Kinds of Cache Misses

    If the cache at level k is empty, then any access of any data object will miss. An empty cache is sometimes referred to as a
cold cache, and misses of this kind are called compulsory misses or cold misses. Cold misses are important because they are often
transient events that might not occur in steady state, after the cache has been warmed up by repeated memory accesses.
    Whenever there is a miss, the cache at level k must implement some placement policy that determines where to place the block 
it has retrieved from level k + 1.  The most flexible placement policy is to allow any block from level k + 1 to be stored in any 
block at level k. For caches high in the memory hierarchy (close to the CPU) that are implemented in hardware and where speed is at 
a premium, this policy is usually too expensive to implement because randomly placed blocks are expensive to locate.
    Thus, hardware caches typically implement a simpler placement policy that restricts a particular block at level k + 1 to a small 
subset (sometimes a singleton) of the blocks at level k. For example, in Figure 6.22, we might decide that a block i at level k + 1 
must be placed in block (i mod 4) at level k. For example, blocks 0, 4, 8, and 12 at level k + 1 would map to block 0 at level k; 
blocks 1, 5, 9, and 13 would map to block 1; and so on.
    Restrictive placement policies of this kind lead to a type of miss known as a conflict miss, in which the cache is large enough 
to hold the referenced data objects, but because they map to the same cache block, the cache keeps missing.

    Cache Management

    As we have noted, the essence of the memory hierarchy is that the storage device at each level is a cache for the next lower 
level. At each level, some form of logic must manage the cache. By this we mean that something has to partition the cache
storage into blocks, transfer blocks between different levels, decide when there are hits and misses, and then deal with them. 
The logic that manages the cache can be hardware, software, or a combination of the two.
    For example, the compiler manages the register file, the highest level of the cache hierarchy. It decides when to issue loads 
when there are misses, and determines which register to store the data in. The caches at levels L1, L2, and L3 are managed entirely 
by hardware logic built into the caches. In a system with virtual memory, the DRAM main memory serves as a cache for data blocks
stored on disk, and is managed by a combination of operating system software and address translation hardware on the CPU. For a machine 
with a distributed file system such as AFS, the local disk serves as a cache that is managed by the AFS client process running on the 
local machine. In most cases, caches operate automatically and do not require any specific or explicit actions from the program.


Type                What cached                 Where cached                Latency(cycles)         Managed by
CPU registers       4-byte or 8-byte words      On-chip CPU registers                   0           Compiler
TLB                 Address translation         On-chip TLB                             0           Hardware MMU
L1 cache            64-byte words               On-chip L1 cache                        4           Hardware
L2 cache            64-byte words               On-chip L2 cache                       10           Hardware
L3 cache            64-byte words               On-chip L3 cache                       50           Hardware
Virtual memory      4-KB pages                  Main memory                           200           Hardware + OS
Buffer cache        Parts of files              Main memory                           200           OS
Disk cache          Disk sectors                Disk controllers                   100000           Controller firmware
Network cache       Parts of files              Local disk                       10000000           NFS client
Browser cache       Web pages                   Local disk                       10000000           Web Browser
Web cache           Web pages                   Remote server disk             1000000000           Web proxy server


    Summary of Memory Hierarcy Concepts

  * Exploiting temporal locality. Because of temporal locality, the same data objects are likely to be reused multiple
    times. Once a data object has been copied into the cache on the first miss, we can expect a number of subseqeunt hits
    on that object. Since the cache is faster than the storage at the next lower level, these subsequent hits can be served
    much faster than the original miss.
  
  * Exploiting spatial locality. Blocks usually contain multiple data objects. Because of spatial locality, we can expect
    that the cost of copying a block after miss will me amortized by subsequent references to other objects within that block.